{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "862752c0",
   "metadata": {},
   "source": [
    "# **COMPLETE GUIDE OF DATA PRE-PROCESSING : WHAT, WHY & HOW TO HANDLE MESSY DATA**\n",
    "\n",
    "### Let's prepare our data for better analysis and modeling - for your information: data scientist and machine learning engineer spend more than 50%  their time on these steps :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dc8794",
   "metadata": {},
   "source": [
    "## **A. Learning Objectives**\n",
    "\n",
    "* **Get a clearer idea of why data preprocessing actually matters**\n",
    "\n",
    "* **Learn how to handle different data preprocessing techniques**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e35acb7",
   "metadata": {},
   "source": [
    "## **B. Background Theory**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc017b00",
   "metadata": {},
   "source": [
    "\n",
    "The quality of your data plays a big role in how accurate your analysis and models turn out. Why? Because raw data usually comes with a bunch of issues—like errors, inconsistencies, or stuff that’s just not useful—which can mess up your results and lead to misleading insights. That’s where data preprocessing comes in. It’s basically the step where you clean and organize your data so it’s ready to be used properly.\n",
    "\n",
    "### **What is Data Pre-Processing ?**\n",
    "\n",
    "Data preprocessing is the process of preparing raw data so it can be used effectively in machine learning models. Think of it like cleaning and organizing ingredients before cooking a meal—if the ingredients are messy or spoiled, the final dish won’t turn out well.\n",
    "\n",
    "### **What It Involves**\n",
    "\n",
    "Here are the key steps in data preprocessing:\n",
    "\n",
    "1. Cleaning the Data\n",
    "    \n",
    "    - Fixing or removing missing values\n",
    "    \n",
    "    - Removing duplicates\n",
    "    \n",
    "    - Correcting errors or inconsistencies\n",
    "    \n",
    "2. Transforming the Data\n",
    "\n",
    "    - Scaling values (e.g., making sure all numbers are on a similar scale)\n",
    "    \n",
    "    - Encoding categories (e.g., turning \"Yes\"/\"No\" into 1/0)\n",
    "    \n",
    "    - Normalizing or standardizing data\n",
    "\n",
    "3. Reducing the Data\n",
    "\n",
    "    - Selecting only the most useful features\n",
    "\n",
    "    - Removing irrelevant or redundant information\n",
    "\n",
    "    - Dimensionality reduction (e.g., using PCA)\n",
    "\n",
    "4. Splitting the Data\n",
    "    \n",
    "    - Dividing into training\n",
    "    \n",
    "    - validation, and test sets\n",
    "\n",
    "\n",
    "### **Why It’s Important in Machine Learning**\n",
    "\n",
    "Machine learning models learn patterns from data. If the data is messy, the model might learn the wrong patterns or get confused. Preprocessing helps:\n",
    "\n",
    "- Improve model accuracy\n",
    "\n",
    "- Speed up training\n",
    "\n",
    "- Avoid errors and bias\n",
    "\n",
    "- Make the model more generalizable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9af8799",
   "metadata": {},
   "source": [
    "## **C. Practical Steps for Effective Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf200765",
   "metadata": {},
   "source": [
    "### **1. Cleaning Data : Handling Missing Value**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08adc033",
   "metadata": {},
   "source": [
    "Missing values are empty spots in your data—like blanks or NaNs—that can mess things up when training a model.\n",
    "\n",
    "- **Why Fix Them?**\n",
    "\n",
    "    Because most machine learning models don’t like missing data. If you ignore them, your model might crash or give bad predictions.\n",
    "\n",
    "- **How to Handle?**\n",
    "\n",
    "    - Drop them if they’re not important\n",
    "    \n",
    "    - Fill them with something like the mean, median, or a default value\n",
    "\n",
    "    - Use forward/backward fill for time-based data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aff4d14",
   "metadata": {},
   "source": [
    "Comparison Table of Handling Missing Value Techniques\n",
    "\n",
    "| Technique                     | When to Use                                                                 | Pros                                                                 | Cons                                                                 | How to Do                                                                                   |\n",
    "|--------------------------------|------------------------------------------------------------------------------|----------------------------------------------------------------------|----------------------------------------------------------------------|------------------------------------------------------------------------------------------------|\n",
    "| **Remove Missing Values**      | - Missing data is rare (<5% of dataset)                                      | - Simple and fast                                                    | - Can lose valuable information                                    | `df.dropna()`                                                                                 |\n",
    "|                                | - Missingness is random                                                      | - No risk of introducing bias from imputation                        | - Can reduce dataset size                                           |                                                                                                |\n",
    "| **Fill NA (Static Value) : mean, median, average and etc**     | - Missing data is more frequent but predictable                              | - Keeps all rows                                                      | - May introduce bias if fill value is not representative            | `df.fillna(0)` or `df.fillna(df['col'].mean())`                                                |\n",
    "|                                | - You have a meaningful constant or statistic (mean, median, mode)           | - Simple to implement                                                 | - Can reduce variance artificially                                  |                                                                                                |\n",
    "| **Forward/Backward Fill**      | - Time series or sequential data where last/next value makes sense           | - Preserves temporal consistency                                      | - Can propagate incorrect values forward/backward                   | `df.fillna(method='ffill')` (forward) or `df.fillna(method='bfill')` (backward)                |\n",
    "|                                | - Small gaps in data that should be smoothed                                 | - Good for short gaps in ordered data                                 | - Not suitable for large gaps                                       |                                                                                                |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3d8076e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have missing values in the following columns: ['Age', 'City', 'Score']\n"
     ]
    }
   ],
   "source": [
    "# Create a sample DataFrame with missing values\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva', 'Frank', 'Grace', 'Hannah', 'Ian', 'Jane',\n",
    "             'Kevin', 'Laura', 'Mike', 'Nina', 'Oscar', 'Paula', 'Quinn', 'Rachel', 'Steve', 'Tina'],\n",
    "    'Age': [25, np.nan, 30, 22, np.nan, 28, 35, 40, np.nan, 29,\n",
    "            31, 27, np.nan, 33, 26, 24, 38, np.nan, 32, 30],\n",
    "    'City': ['Jakarta', 'Medan', np.nan, 'Bandung', 'Surabaya', 'Jakarta', 'Medan', 'Bandung', 'Surabaya', np.nan,\n",
    "             'Jakarta', 'Medan', 'Bandung', np.nan, 'Surabaya', 'Jakarta', 'Medan', 'Bandung', 'Surabaya', 'Jakarta'],\n",
    "    'Score': [88, 92, np.nan, 85, 90, 87, np.nan, 91, 89, 86,\n",
    "              93, np.nan, 84, 88, 90, 85, 92, 89, np.nan, 87]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# check column with missing values\n",
    "columns_with_missing = df.columns[df.isnull().any()].tolist() ; \n",
    "\n",
    "print(f\"we have missing values in the following columns:\", columns_with_missing);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04c26d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name     0\n",
      "Age      5\n",
      "City     0\n",
      "Score    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. DROP Missing Values\n",
    "# from the info above, we can see that the InternetService column has missing values\n",
    "\n",
    "df_drop = df.dropna(subset=['City', 'Score'])\n",
    "null_counts = df_drop.isnull().sum()  # check null values after dropping rows\n",
    "print(null_counts) # display the DataFrame after dropping rows with missing values, now we have no missing values in the 'City' and 'Score' columns but still have missing values in the 'Age' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b540f04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Name   Age      City  Score  Age_mean  Score_median City_filled\n",
      "0     Alice  25.0   Jakarta   88.0      25.0          88.0     Jakarta\n",
      "1       Bob   NaN     Medan   92.0      30.0          92.0       Medan\n",
      "2   Charlie  30.0       NaN    NaN      30.0          88.5     Unknown\n",
      "3     David  22.0   Bandung   85.0      22.0          85.0     Bandung\n",
      "4       Eva   NaN  Surabaya   90.0      30.0          90.0    Surabaya\n",
      "5     Frank  28.0   Jakarta   87.0      28.0          87.0     Jakarta\n",
      "6     Grace  35.0     Medan    NaN      35.0          88.5       Medan\n",
      "7    Hannah  40.0   Bandung   91.0      40.0          91.0     Bandung\n",
      "8       Ian   NaN  Surabaya   89.0      30.0          89.0    Surabaya\n",
      "9      Jane  29.0       NaN   86.0      29.0          86.0     Unknown\n",
      "10    Kevin  31.0   Jakarta   93.0      31.0          93.0     Jakarta\n",
      "11    Laura  27.0     Medan    NaN      27.0          88.5       Medan\n",
      "12     Mike   NaN   Bandung   84.0      30.0          84.0     Bandung\n",
      "13     Nina  33.0       NaN   88.0      33.0          88.0     Unknown\n",
      "14    Oscar  26.0  Surabaya   90.0      26.0          90.0    Surabaya\n",
      "15    Paula  24.0   Jakarta   85.0      24.0          85.0     Jakarta\n",
      "16    Quinn  38.0     Medan   92.0      38.0          92.0       Medan\n",
      "17   Rachel   NaN   Bandung   89.0      30.0          89.0     Bandung\n",
      "18    Steve  32.0  Surabaya    NaN      32.0          88.5    Surabaya\n",
      "19     Tina  30.0   Jakarta   87.0      30.0          87.0     Jakarta\n"
     ]
    }
   ],
   "source": [
    "# 2. Fill missing numerical values with mean\n",
    "\n",
    "df['Age_mean'] = df['Age'].fillna(df['Age'].mean())\n",
    "\n",
    "# Fill missing numerical values with median\n",
    "df['Score_median'] = df['Score'].fillna(df['Score'].median())\n",
    "\n",
    "# Fill missing categorical/string values with a default : Unknown\n",
    "df['City_filled'] = df['City'].fillna('Unknown')\n",
    "\n",
    "print(df)   # Display the DataFrame after filling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c25c137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Name  Age_ffill  Age_bfill\n",
      "0     Alice       25.0       25.0\n",
      "1       Bob       25.0       30.0\n",
      "2   Charlie       30.0       30.0\n",
      "3     David       22.0       22.0\n",
      "4       Eva       22.0       28.0\n",
      "5     Frank       28.0       28.0\n",
      "6     Grace       35.0       35.0\n",
      "7    Hannah       40.0       40.0\n",
      "8       Ian       40.0       29.0\n",
      "9      Jane       29.0       29.0\n",
      "10    Kevin       31.0       31.0\n",
      "11    Laura       27.0       27.0\n",
      "12     Mike       27.0       33.0\n",
      "13     Nina       33.0       33.0\n",
      "14    Oscar       26.0       26.0\n",
      "15    Paula       24.0       24.0\n",
      "16    Quinn       38.0       38.0\n",
      "17   Rachel       38.0       32.0\n",
      "18    Steve       32.0       32.0\n",
      "19     Tina       30.0       30.0\n"
     ]
    }
   ],
   "source": [
    "# 3. Forward/Backward Fill\n",
    "\n",
    "df['Age_ffill'] = df['Age'].ffill()  # Forward fill\n",
    "df['Age_bfill'] = df['Age'].bfill()  # Backward fill\n",
    "\n",
    "print(df[['Name','Age_ffill', 'Age_bfill']])  # Display the DataFrame with forward and backward filled values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a86536",
   "metadata": {},
   "source": [
    "### **2 Transforming Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78fbedf",
   "metadata": {},
   "source": [
    "- **What is Transforming Data**\n",
    "\n",
    "    Before we feed data into a machine learning model, we often need to clean it up and reshape it a bit. That’s what data transformation is all about — making sure the data is in the right format and scale so the model can actually learn from it.\n",
    "\n",
    "    Think of it like prepping ingredients before cooking. You wouldn’t throw whole vegetables into a blender without chopping them first, right?\n",
    "\n",
    "- **Why we need to Transform our Data**\n",
    "\n",
    "  Machine learning models are picky. They work best when:\n",
    "\n",
    "  - a. Numbers are on similar scales  \n",
    "  - b. Categories are turned into something they can understand (like numbers)  \n",
    "  - c. Data is clean, consistent, and standardized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6694fb",
   "metadata": {},
   "source": [
    "**How Do We Transform Data?**\n",
    "\n",
    "| Technique Name     | Definition                                                                 | Popular Methods                                      | When to Use                                                                 | Pros                                                                 | Cons                                                                 | How to Do (Python Code)                                                                 |\n",
    "|--------------------|-----------------------------------------------------------------------------|------------------------------------------------------|------------------------------------------------------------------------------|----------------------------------------------------------------------|----------------------------------------------------------------------|------------------------------------------------------------------------------------------|\n",
    "| Scaling Values      | Adjusts numerical values to a common scale without changing distribution   | Min-Max Scaling, Robust Scaling, MaxAbs Scaling      | When features have different ranges (e.g., 0–1 vs 0–1000)                    | Helps models converge faster and treat features equally              | Doesn’t handle outliers well (MinMax), may distort data              | `MinMaxScaler().fit_transform(data)`<br>`RobustScaler().fit_transform(data)`            |\n",
    "| Encoding Categories | Converts categorical data into numerical format                            | One-Hot Encoding, Label Encoding, Ordinal Encoding   | When you have non-numeric features like \"Yes\"/\"No\", \"Red\"/\"Blue\"            | Makes categorical data usable by ML models                           | Can increase dimensionality (One-Hot), may imply order (Label)       | `LabelEncoder().fit_transform(data)`<br>`pd.get_dummies(data)`<br>`OrdinalEncoder()`    |\n",
    "| Normalizing         | Scales data to a fixed range, usually [0, 1]                               | Min-Max Normalization, L2 Normalization, MaxAbs      | When data needs to be bounded or when using distance-based models           | Keeps data within a consistent range                                 | Sensitive to outliers                                                | `MinMaxScaler().fit_transform(data)`<br>`normalize(data, norm='l2')`                    |\n",
    "| Standardizing       | Centers data around mean 0 and std dev 1                                   | Z-score Standardization, Robust Scaling, PowerTransform | When data has varying distributions or outliers                             | Handles outliers better than normalization                           | Assumes Gaussian distribution (may not fit all data)                 | `StandardScaler().fit_transform(data)`<br>`RobustScaler().fit_transform(data)`          |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eab82c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Age",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Income",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Subscribed",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Income_scaled",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Subscribed_encoded",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Age_norm",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Income_norm",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Age_std",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Income_std",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "c8232afb-9196-4a5f-98a4-1d908b065332",
       "rows": [
        [
         "0",
         "25",
         "50000",
         "Yes",
         "0.0",
         "1",
         "0.0004999999375000116",
         "0.9999998750000234",
         "-1.382872443578988",
         "-1.283777909595512"
        ],
        [
         "1",
         "32",
         "64000",
         "No",
         "0.14",
         "0",
         "0.0004999999375000118",
         "0.9999998750000234",
         "-0.85677966613046",
         "-0.8997417827934359"
        ],
        [
         "2",
         "47",
         "120000",
         "Yes",
         "0.7000000000000002",
         "1",
         "0.0003916666366252928",
         "0.99999992329862",
         "0.2705619998306717",
         "0.6364027244148692"
        ],
        [
         "3",
         "51",
         "100000",
         "No",
         "0.5",
         "0",
         "0.0005099999336745129",
         "0.9999998699500253",
         "0.5711864440869735",
         "0.08777968612618886"
        ],
        [
         "4",
         "62",
         "150000",
         "Yes",
         "1.0000000000000002",
         "1",
         "0.000413333298025486",
         "0.9999999145777888",
         "1.3979036657918034",
         "1.4593372818478898"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Income</th>\n",
       "      <th>Subscribed</th>\n",
       "      <th>Income_scaled</th>\n",
       "      <th>Subscribed_encoded</th>\n",
       "      <th>Age_norm</th>\n",
       "      <th>Income_norm</th>\n",
       "      <th>Age_std</th>\n",
       "      <th>Income_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>50000</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.382872</td>\n",
       "      <td>-1.283778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>64000</td>\n",
       "      <td>No</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.856780</td>\n",
       "      <td>-0.899742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47</td>\n",
       "      <td>120000</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.270562</td>\n",
       "      <td>0.636403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51</td>\n",
       "      <td>100000</td>\n",
       "      <td>No</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.571186</td>\n",
       "      <td>0.087780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "      <td>150000</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.397904</td>\n",
       "      <td>1.459337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Income Subscribed  Income_scaled  Subscribed_encoded  Age_norm  \\\n",
       "0   25   50000        Yes           0.00                   1  0.000500   \n",
       "1   32   64000         No           0.14                   0  0.000500   \n",
       "2   47  120000        Yes           0.70                   1  0.000392   \n",
       "3   51  100000         No           0.50                   0  0.000510   \n",
       "4   62  150000        Yes           1.00                   1  0.000413   \n",
       "\n",
       "   Income_norm   Age_std  Income_std  \n",
       "0          1.0 -1.382872   -1.283778  \n",
       "1          1.0 -0.856780   -0.899742  \n",
       "2          1.0  0.270562    0.636403  \n",
       "3          1.0  0.571186    0.087780  \n",
       "4          1.0  1.397904    1.459337  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the packages\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder, normalize\n",
    "\n",
    "# sample data for scaling, encoding, normalizing, and standardizing\n",
    "data = {\n",
    "    'Age': [25, 32, 47, 51, 62],\n",
    "    'Income': [50000, 64000, 120000, 100000, 150000],\n",
    "    'Subscribed': ['Yes', 'No', 'Yes', 'No', 'Yes']\n",
    "}\n",
    "\n",
    "df_1 = pd.DataFrame(data)\n",
    "\n",
    "# 1. Scaling (Min-Max)\n",
    "scaler = MinMaxScaler()\n",
    "df_1['Income_scaled'] = scaler.fit_transform(df_1[['Income']])\n",
    "\n",
    "# 2. Encoding (Label Encoding)\n",
    "encoder = LabelEncoder()\n",
    "df_1['Subscribed_encoded'] = encoder.fit_transform(df_1['Subscribed'])\n",
    "\n",
    "# 3. Normalizing (L2 norm)\n",
    "df_1[['Age_norm', 'Income_norm']] = normalize(df_1[['Age', 'Income']], norm='l2')\n",
    "\n",
    "# 4. Standardizing (Z-score)\n",
    "standardizer = StandardScaler()\n",
    "df_1[['Age_std', 'Income_std']] = standardizer.fit_transform(df_1[['Age', 'Income']])\n",
    "\n",
    "# Show the final DataFrame with all transformations\n",
    "df_1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1043347",
   "metadata": {},
   "source": [
    "### **3. Reducing the Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030a5abe",
   "metadata": {},
   "source": [
    "**What is Data Reduction ?**\n",
    "\n",
    "Data reduction is all about simplifying your dataset without losing the important stuff. In machine learning, we often deal with tons of features — but not all of them are useful. Some might be irrelevant, redundant, or just noise.\n",
    "\n",
    "So, reducing the data means:\n",
    "\n",
    "  - Picking only the most useful features\n",
    "  - Dropping the ones that don’t help\n",
    "  - Compressing the data into fewer dimensions (like using PCA)\n",
    "\n",
    "**Why Do We Need to Reduce Data ?**\n",
    "\n",
    "More data isn’t always better. Here’s why reducing it matters:\n",
    "  - Improves model performance: Less noise = better predictions\n",
    "  - Speeds up training: Fewer features = faster computation\n",
    "  - Avoids overfitting: Too many features can make the model memorize instead of generalize\n",
    "  - Makes interpretation easier: Simpler models are easier to explain\n",
    "\n",
    "> In short, reducing data helps us build smarter, faster, and more reliable models.\n",
    "\n",
    "**How Do We Reduce Data ?**\n",
    "\n",
    "1. Feature Selection\n",
    "    - Choose only the features that actually contribute to the prediction. You can do this manually (based on domain knowledge) or automatically using techniques like:\n",
    "    - Correlation analysis\n",
    "    - Recursive Feature Elimination (RFE)\n",
    "    - Feature importance from models (e.g., Random Forest)\n",
    "\n",
    "2. Removing Irrelevant or Redundant Info\n",
    "    - Drop columns that don’t vary much, are duplicates, or have too many missing values. This helps clean up the dataset and reduce clutter.\n",
    "\n",
    "3. Dimensionality Reduction\n",
    "    - Use techniques like Principal Component Analysis (PCA) to compress the data into fewer dimensions while keeping most of the information.\n",
    "\n",
    "> **FINAL THOUGHT** : Reducing data isn’t about throwing things away — it’s about being smart with what you keep. By focusing on the most meaningful features, we make our models more efficient, accurate, and easier to understand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cda024f",
   "metadata": {},
   "source": [
    "| Technique Name        | Definition                                                                 | Popular Methods                          | When to Use                                                                 | Pros                                                                 | Cons                                                                 | How to Do (Python Code)                                                                 |\n",
    "|-----------------------|-----------------------------------------------------------------------------|------------------------------------------|------------------------------------------------------------------------------|----------------------------------------------------------------------|----------------------------------------------------------------------|------------------------------------------------------------------------------------------|\n",
    "| Feature Selection     | Choosing only the most relevant features for the model                     | Filter Methods, Wrapper Methods, Embedded Methods | When you have many features but only a few are useful                        | Improves model accuracy and speed                                     | May miss interactions between features                                | `from sklearn.feature_selection import SelectKBest, f_classif`<br>`selected = SelectKBest(score_func=f_classif, k=3).fit_transform(X, y)`<br><br>`from sklearn.feature_selection import RFE`<br>`rfe = RFE(estimator=LogisticRegression(), n_features_to_select=3).fit(X, y)`<br><br>`model.feature_importances_` (e.g., from RandomForest) |\n",
    "| Removing Irrelevant/Redundant Info | Dropping features that are constant, duplicated, or highly correlated | Variance Threshold, Correlation Matrix, Manual Inspection | When features have low variance or are duplicates                          | Simplifies the dataset                                                | Requires domain knowledge or manual effort                            | `from sklearn.feature_selection import VarianceThreshold`<br>`vt = VarianceThreshold(threshold=0.1).fit_transform(X)`<br><br>`df.drop_duplicates()`<br><br>`df.drop(columns=df.columns[df.corr().abs().gt(0.95).any()], inplace=True)` |\n",
    "| Dimensionality Reduction | Compressing data into fewer dimensions while preserving structure        | PCA, t-SNE, UMAP                          | When data has many features and you want to visualize or reduce noise       | Reduces overfitting, improves visualization                           | Can lose interpretability and information                             | `from sklearn.decomposition import PCA`<br>`pca = PCA(n_components=2)`<br>`X_pca = pca.fit_transform(X)`<br><br>`from sklearn.manifold import TSNE`<br>`tsne = TSNE(n_components=2)`<br>`X_tsne = tsne.fit_transform(X)`<br><br>`import umap`<br>`reducer = umap.UMAP(n_components=2)`<br>`X_umap = reducer.fit_transform(X)` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cbfe83",
   "metadata": {},
   "source": [
    "> We Explore this Technique on Next Article :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c0fd88",
   "metadata": {},
   "source": [
    "### **4. Splitting the Data: What, Why, and How**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2ea3de",
   "metadata": {},
   "source": [
    "**What is Data Splitting?**\n",
    "\n",
    "Data splitting is the process of dividing your dataset into separate parts so you can train, validate, and test your machine learning model properly. It’s like setting aside different portions of your data for different jobs:\n",
    "\n",
    "  - Training set: Used to teach the model\n",
    "  - Validation set: Used to tune the model (e.g., choosing the best parameters)\n",
    "  - Test set: Used to evaluate how well the model performs on unseen data\n",
    "\n",
    "**Why Do We Split the Data?**\n",
    "\n",
    "We split the data to make sure our model isn’t just memorizing the training data — we want it to generalize well to new, unseen data.\n",
    "\n",
    "Here’s why it matters:\n",
    "  - Avoid overfitting: Helps detect if the model is just memorizing patterns\n",
    "  - Tune hyperparameters safely: Validation set helps us adjust settings without touching the test set\n",
    "  - Measure real-world performance: The test set gives us a realistic idea of how the model will perform in production\n",
    "\n",
    "> Without proper splitting, we risk building a model that looks great during training but fails miserably in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7179c9e6",
   "metadata": {},
   "source": [
    "**Data Splitting Method**\n",
    "\n",
    "| Method Name              | Description                                                                 | Python Code                                                                 | Pros                                                                                 | Cons                                                                                   |\n",
    "|--------------------------|-----------------------------------------------------------------------------|------------------------------------------------------------------------------|--------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------|\n",
    "| Train/Test Split         | Splits data into training and testing sets                                  | `train_test_split(X, y, test_size=0.2)`                                     | Simple, fast, good for large datasets                                               | No validation set, risk of overfitting if test set is too small                        |\n",
    "| Train/Validation/Test    | Splits data into training, validation, and testing sets                     | `train_test_split()` twice                                                  | Allows tuning and final evaluation                                                  | More complex, reduces training data size                                               |\n",
    "| K-Fold Cross Validation  | Splits data into *k* folds, trains on *k-1*, tests on 1                     | `KFold(n_splits=5).split(X)`                                                | Robust evaluation, uses all data for training/testing                               | Computationally expensive, may not preserve class balance                              |\n",
    "| Stratified K-Fold        | Like K-Fold but preserves class distribution                                | `StratifiedKFold(n_splits=5).split(X, y)`                                   | Better for imbalanced classification problems                                       | Slightly more complex to implement                                                     |\n",
    "| Leave-One-Out (LOO)      | Each sample is used once as test, rest as training                          | `LeaveOneOut().split(X)`                                                    | Very precise, uses all data                                                         | Very slow for large datasets                                                           |\n",
    "| Time Series Split        | Splits data sequentially, respecting time order                             | `TimeSeriesSplit(n_splits=5).split(X)`                                      | Prevents data leakage, good for forecasting                                         | Doesn’t shuffle, may not generalize well                                               |\n",
    "| Group K-Fold             | Ensures same group isn’t in both train and test                             | `GroupKFold(n_splits=5).split(X, y, groups)`                                | Prevents data leakage across related samples                                        | Requires group labels, may reduce randomness                                           |\n",
    "| Shuffle Split (Monte Carlo) | Randomly splits data multiple times for repeated evaluation             | `ShuffleSplit(n_splits=10, test_size=0.2).split(X, y)`                      | Flexible, good for repeated random testing                                          | Doesn’t guarantee stratification or group separation                                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3862c6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
